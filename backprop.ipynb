{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><h3>*-----ASSUMING THAT THE FORWARD PROPAGATION HAS ALREADY HAPPENED-----*</h3><br/>\n",
    "<b>BACK-PROP</b> :\n",
    "<i>\n",
    "<b>MAIN TASK</b> :<li>\n",
    "    <ol>How to learn multiple layers of features ?</ol>\n",
    "    <ol>How to learn weights of hidden units? [They are called hidden units for a reason]</ol>\n",
    "</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><b>OTHER WAYS TO DO IT AND WHY BACKPROPAGATION IS A BETTER WAY</b></i>\n",
    "1.<h4> Perturb weights manually , and then check the output . See how well they fare against their respective target</h4><br/>\n",
    "<b>Problem</b>  :<b>TOO INEFFICIENT</b> .<br/>As the number of neurons/layers increase the ways of Perturbing a weight is increased rapidly . for a 2 hidden layer network ,with 4 neurons each it would take \n",
    "*input_size*4*4*output_size* number of ways to get it right \n",
    "\n",
    "<h5>and even to get this going </h5>: \n",
    "<i>There is a necessity to have a loop that does the task of changing the features without looking into the task</i>\n",
    "<i>without trial and error .automated one ofcourse.</i>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>ADVANTAGES OF USING BACK_PROPAGATION</h3>:\n",
    "    1. No need to know what hidden units are doing . Instead of using desired activations to train hidden units use derivatives with-respect-to hidden activities\n",
    "    2. Each activation can have  different effects on different output units* \n",
    "    3. error derivatives tells how fast error changes with respect to the hidden activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"eins.jpeg\" width=\"400\" height=\"1000\"  >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"loss.jpeg\" width=400 height=1000>",
    "<h3>A loss function contain </h3>\n",
    "<h3>y<sub>j,c</sub> the output and t<sub>j,c</sub> the target</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"sigmoid.jpeg\" width=400 height=1000>",
    "<h2>And a sigmoid activation function </h2>:<br/><b> This function just gives out the squashed value of an input such that the ouput lies between 0 and 1 </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"work.jpeg\" width=400 height=1000>",
    "<h3>The work of backprop goes something like this  making use of the chain rule  ,which gives the correspondance</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>STEPS</h4>\n",
    "\n",
    "<h4>STEP 1</h4><i>Convert the discrepancy between each operation and its target value into an error derivative</i><br></br>\n",
    "<h4>STEP 2</h4><i>Compute error derivatives in each hidden layer from error derivatives from the layer before [or above ]</i>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>The above steps has to go on from the output and then end at the input of the network layer</i> <br></br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>This will be implemented in the following way</h5>\n",
    "       <i> At every neuron there will be two operations going on .<br> 1. Calculation of local gradient and calculation of the external gradient <br>2.Multiplying the gradients from the upper layers and distributing the gradients<br>\n",
    "<b>Local gradient </b>: Are those which are the inputs to that particular neuron . As and when the neuron gets the input it calculates the local gradient<br>\n",
    "<b>external gradient</b>: Are those gradients that are obtained by the upper layer neurons  ,which they get by multiplying a output_value and calculating its gradient and multiplying it with the local derivatives in the neuron itself . \n",
    "<br><br>\n",
    "local gradients are <b>JACOBIAN MATRICES</b><br>\n",
    "external gradients are <b>VECTORS</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.0.dev20190517'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.5972e-01, 5.0503e-01, 3.2164e-01],\n",
       "        [6.3807e-04, 7.2887e-01, 5.2593e-02],\n",
       "        [1.6526e-01, 5.3888e-01, 6.6202e-01]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.rand_like(a , requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*in torch specification has to be made whether to have the gradient calculated during backprop or not that is done by giving a boolean value to requires_grad*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8815, 0.4754, 0.1780],\n",
       "        [0.7745, 0.7840, 0.9789],\n",
       "        [0.7123, 0.1485, 0.1391]], requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([-0.1626,  0.0620], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0822, -0.2454,  0.1748],\n",
      "        [ 0.1505, -0.5158,  0.5427]], requires_grad=True)\n",
      "loss:  0.8906837701797485\n",
      "1\n",
      "loss after 1 step optimisation  0.8906837701797485\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "x = torch.randn(10 ,3)\n",
    "y = torch.randn(10 , 2)\n",
    "\n",
    "linear = nn.Linear(3,2)\n",
    "\n",
    "print(linear.bias)\n",
    "print(linear.weight)\n",
    "criterion = nn.MSELoss()\n",
    "optimiser = torch.optim.RMSprop(linear.parameters() , lr=0.01)\n",
    "\n",
    "pred = linear(x)\n",
    "loss= criterion(pred , y)\n",
    "#USAGE OF .item() is needed because loss is a tensor and to get the individual value in it .\n",
    "print('loss: ' , loss.item())\n",
    "print('1' if  loss.grad_fn.requires_grad==True else '0')\n",
    "loss.backward()#Does the backpropagation \n",
    "\n",
    "\n",
    "optimiser.zero_grad()\n",
    "optimiser.step()#updates \n",
    "pred = linear(x)\n",
    "loss = criterion(pred ,y)\n",
    "print('loss after 1 step optimisation ', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
