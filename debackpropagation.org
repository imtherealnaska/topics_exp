*ASSUMING THAT THE FORWARD PROPAGATION HAS ALREADY HAPPENED*
*BACK-PROP* :

*MAIN TASK* : 1.How to learn multiple layers of features ?
              2.How to learn weights of hidden units? [They are called hidden units for a reason]

/other ways to do it and why backprop is a better option/
1. Perturb weights manually , and then check the output . See how well they fare against their respective target
##Problem  :TOO INEFFICIENT .As the number of neurons/layers increase the ways of Perturbing a weight is increased rapidly . for a 2 hidden layer network ,with 4 neurons each it would take 
*input_size*4*4*output_size* number of ways to get it right 

##and even to get this going : 
/There is a necessity to have a loop that does the task of changing the features without looking into the task/ 
/without trial and error .automated one ofcourse./

*USING BACK_PROP*:
1.*No need to know what hidden units are doing* . Instead of using desired activations to train hidden units ,use derivatives with-respect-to hidden activities
2.*Each activation can have  different effects on different output units* 
3.*error derivatives tells how fast error chnages with respect to the hidden activity*









